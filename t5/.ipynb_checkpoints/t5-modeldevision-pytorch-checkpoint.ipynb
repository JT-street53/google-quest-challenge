{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "T5-base: 5 SPLIT and 5 FOLD CV to predict. <br>\n",
    "### Versions\n",
    "- T5MD004<br>\n",
    "CV : 0.40192<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys, gc, os, re, tqdm, datetime, random, itertools, copy, math, html\n",
    "\n",
    "sys.path.extend([\n",
    "    '../../input/sacremoses/', \n",
    "    '../../input/transformers/'\n",
    "])\n",
    "import sacremoses, transformers\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, models, transforms\n",
    "from sklearn.utils import shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = 'T5MD004'\n",
    "\n",
    "LOCAL_PATH = '../../input/google-quest-challenge'\n",
    "MODEL_PRETRAINED_WEIGHTS_PATH = '../../input/t5-base-huggingface-weights'\n",
    "WEIGHT_PATH = '../../input/weights'\n",
    "N_SPLIT = 5\n",
    "FOLD_ID = [0,1,2,3,4]\n",
    "SEED = 9253\n",
    "MAX_SEQUENCE_LENGTH = 512\n",
    "BATCH_SIZE = 4\n",
    "BATCH_ACCUMULATION_COUNT = 8\n",
    "EPOCHS = 100\n",
    "EPOCH_RELEASE = 2\n",
    "EARLY_STOPPING = 3\n",
    "LR = 1e-3\n",
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "NUM_WORKERS = 4\n",
    "TRAINING = True\n",
    "\n",
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(LOCAL_PATH+'/train.csv')\n",
    "print(train.shape)\n",
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(LOCAL_PATH+'/test.csv')\n",
    "print(test.shape)\n",
    "test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(LOCAL_PATH+'/sample_submission.csv')\n",
    "print(sample_submission.shape)\n",
    "sample_submission.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing\n",
    "Credit to [Bert-base TF2.0 (now Huggingface transformer)](https://www.kaggle.com/akensert/bert-base-tf2-0-now-huggingface-transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shutil import copyfile\n",
    "from transformers.tokenization_utils import PreTrainedTokenizer\n",
    "\n",
    "SPIECE_UNDERLINE = \"‚ñÅ\"\n",
    "VOCAB_FILES_NAMES = {\"vocab_file\": \"spiece.model\"}\n",
    "PRETRAINED_VOCAB_FILES_MAP = {\n",
    "    \"vocab_file\": {\n",
    "        \"t5-small\": \"https://s3.amazonaws.com/models.huggingface.co/bert/t5-spiece.model\",\n",
    "        \"t5-base\": \"https://s3.amazonaws.com/models.huggingface.co/bert/t5-spiece.model\",\n",
    "        \"t5-large\": \"https://s3.amazonaws.com/models.huggingface.co/bert/t5-spiece.model\",\n",
    "        \"t5-3b\": \"https://s3.amazonaws.com/models.huggingface.co/bert/t5-spiece.model\",\n",
    "        \"t5-11b\": \"https://s3.amazonaws.com/models.huggingface.co/bert/t5-spiece.model\",\n",
    "    }\n",
    "}\n",
    "PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {\n",
    "    \"t5-small\": 512,\n",
    "    \"t5-base\": 512,\n",
    "    \"t5-large\": 512,\n",
    "    \"t5-3b\": 512,\n",
    "    \"t5-11b\": 512,\n",
    "}\n",
    "\n",
    "class T5Tokenizer(PreTrainedTokenizer):\n",
    "    \"\"\"\n",
    "        SentencePiece based tokenizer. Peculiarities:\n",
    "            - requires `SentencePiece <https://github.com/google/sentencepiece>`_\n",
    "            - `extra_ids` add a number of extra ids added to the end of the vocabulary for use as sentinels.\n",
    "                These tokens are accessible as `<extra_id_{%d}>` where `{%d}` is a number between 0 and extra_ids-1.\n",
    "                Extra tokens are indexed from the end of the vocabulary up to beginnning (<extra_id_0> is the last token in the vocabulary)\n",
    "                (like in T5 preprocessing\n",
    "                see: https://github.com/google-research/text-to-text-transfer-transformer/blob/9fd7b14a769417be33bc6c850f9598764913c833/t5/data/preprocessors.py#L2117)\n",
    "    \"\"\"\n",
    "\n",
    "    vocab_files_names = VOCAB_FILES_NAMES\n",
    "    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\n",
    "    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_file,\n",
    "        eos_token=\"</s>\",\n",
    "        unk_token=\"<unk>\",\n",
    "        pad_token=\"<pad>\",\n",
    "        extra_ids=100,\n",
    "        additional_special_tokens=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        # Add extra_ids to the special token list\n",
    "        if extra_ids > 0:\n",
    "            if additional_special_tokens is None:\n",
    "                additional_special_tokens = []\n",
    "            additional_special_tokens.extend([\"<extra_id_{}>\".format(i) for i in range(extra_ids)])\n",
    "\n",
    "        super().__init__(\n",
    "            eos_token=eos_token,\n",
    "            unk_token=unk_token,\n",
    "            pad_token=pad_token,\n",
    "            additional_special_tokens=additional_special_tokens,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            import sentencepiece as spm\n",
    "        except ImportError:\n",
    "            logger.warning(\n",
    "                \"You need to install SentencePiece to use T5Tokenizer:\"\n",
    "                \"https://github.com/google/sentencepiece\"\n",
    "                \"pip install sentencepiece\"\n",
    "            )\n",
    "            raise\n",
    "\n",
    "        self.vocab_file = vocab_file\n",
    "        self._extra_ids = extra_ids\n",
    "\n",
    "        self.sp_model = spm.SentencePieceProcessor()\n",
    "        self.sp_model.Load(vocab_file)\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return self.sp_model.get_piece_size() + self._extra_ids\n",
    "\n",
    "    def __getstate__(self):\n",
    "        state = self.__dict__.copy()\n",
    "        state[\"sp_model\"] = None\n",
    "        return state\n",
    "\n",
    "    def __setstate__(self, d):\n",
    "        self.__dict__ = d\n",
    "        try:\n",
    "            import sentencepiece as spm\n",
    "        except ImportError:\n",
    "            logger.warning(\n",
    "                \"You need to install SentencePiece to use XLNetTokenizer: https://github.com/google/sentencepiece\"\n",
    "                \"pip install sentencepiece\"\n",
    "            )\n",
    "            raise\n",
    "        self.sp_model = spm.SentencePieceProcessor()\n",
    "        self.sp_model.Load(self.vocab_file)\n",
    "\n",
    "    def _tokenize(self, text, sample=False):\n",
    "        \"\"\" Take as input a string and return a list of strings (tokens) for words/sub-words\n",
    "        \"\"\"\n",
    "        if not sample:\n",
    "            pieces = self.sp_model.EncodeAsPieces(text)\n",
    "        else:\n",
    "            pieces = self.sp_model.SampleEncodeAsPieces(text, 64, 0.1)\n",
    "        return pieces\n",
    "\n",
    "    def _convert_token_to_id(self, token):\n",
    "        \"\"\" Converts a token (str) in an id using the vocab. \"\"\"\n",
    "        if token.startswith(\"<extra_id_\"):\n",
    "            match = re.match(r\"<extra_id_(\\d+)>\", token)\n",
    "            num = int(match.group(1))\n",
    "            return self.vocab_size - num - 1\n",
    "        return self.sp_model.piece_to_id(token)\n",
    "\n",
    "    def _convert_id_to_token(self, index):\n",
    "        \"\"\"Converts an index (integer) in a token (str) using the vocab.\"\"\"\n",
    "        if index < self.sp_model.get_piece_size():\n",
    "            token = self.sp_model.IdToPiece(index)\n",
    "        else:\n",
    "            token = \"<extra_id_{}>\".format(self.vocab_size - 1 - index)\n",
    "        return token\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        \"\"\" Converts a sequence of tokens (string) in a single string. \"\"\"\n",
    "        out_string = self.sp_model.decode_pieces(tokens)\n",
    "        return out_string\n",
    "\n",
    "    def save_vocabulary(self, save_directory):\n",
    "        \"\"\" Save the sentencepiece vocabulary (copy original file) and special tokens file\n",
    "            to a directory.\n",
    "        \"\"\"\n",
    "        if not os.path.isdir(save_directory):\n",
    "            logger.error(\"Vocabulary path ({}) should be a directory\".format(save_directory))\n",
    "            return\n",
    "        out_vocab_file = os.path.join(save_directory, VOCAB_FILES_NAMES[\"vocab_file\"])\n",
    "\n",
    "        if os.path.abspath(self.vocab_file) != os.path.abspath(out_vocab_file):\n",
    "            copyfile(self.vocab_file, out_vocab_file)\n",
    "\n",
    "        return (out_vocab_file,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "# Define Tokenizer and some utility variables\n",
    "##########################\n",
    "\n",
    "tokenizer = T5Tokenizer(\n",
    "    vocab_file = MODEL_PRETRAINED_WEIGHTS_PATH + '/t5-spiece.model'\n",
    ")\n",
    "tokenizer.cls_token = '[CLS]'\n",
    "tokenizer.sep_token = '[SEP]'\n",
    "TOKEN_CLS = tokenizer.cls_token\n",
    "TOKEN_SEP = tokenizer.sep_token\n",
    "SEP_TOKEN_ID = tokenizer.sep_token_id\n",
    "\n",
    "ADD_TOKEN_LIST = [\n",
    "    '[TITLE]', \n",
    "    '[BODY]',\n",
    "    '[CATEGORY]',\n",
    "    '[DOMAIN]',\n",
    "    '[HOST]',\n",
    "    '[category:LIFE_ARTS]', \n",
    "    '[category:CULTURE]', \n",
    "    '[category:SCIENCE]', \n",
    "    '[category:STACKOVERFLOW]', \n",
    "    '[category:TECHNOLOGY]', \n",
    "    '[domain:stackexchange]',\n",
    "    '[domain:stackoverflow]',\n",
    "    '[domain:askubuntu]',\n",
    "    '[domain:serverfault]',\n",
    "    '[domain:superuser]',\n",
    "    '[domain:mathoverflow]',\n",
    "    '\\n'\n",
    "] + list(train.host.unique())\n",
    "num_added_tokens = tokenizer.add_tokens(ADD_TOKEN_LIST)\n",
    "print('Number of Tokens Added : ', num_added_tokens)\n",
    "\n",
    "output_categories_question = list(train.columns[11:32])\n",
    "output_categories_answer = list(train.columns[32:])\n",
    "\n",
    "train.question_title= train.question_title.apply(html.unescape)\n",
    "train.question_body = train.question_body.apply(html.unescape)\n",
    "train.answer        = train.answer.apply(html.unescape)\n",
    "test.question_title = test.question_title.apply(html.unescape)\n",
    "test.question_body  = test.question_body.apply(html.unescape)\n",
    "test.answer         = test.answer.apply(html.unescape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "# Define Datasets and Dataloaders\n",
    "##########################\n",
    "from math import floor, ceil\n",
    "\n",
    "class QuestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, target_columns, max_sequence_length=MAX_SEQUENCE_LENGTH, \n",
    "                 target_level=0, train_mode=True, labeled=True):\n",
    "        '''\n",
    "        target_level\n",
    "            0 : question only\n",
    "            1 : answer only\n",
    "        '''\n",
    "        self.df = df\n",
    "        self.target_columns = target_columns\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.target_level = target_level\n",
    "        self.train_mode = train_mode\n",
    "        self.labeled = labeled\n",
    "        self.tokenizer = T5Tokenizer(\n",
    "            vocab_file = MODEL_PRETRAINED_WEIGHTS_PATH + '/t5-spiece.model'\n",
    "        )\n",
    "        self.tokenizer.add_tokens(ADD_TOKEN_LIST)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        token_ids, seg_ids = self.get_token_ids(row)\n",
    "        if self.labeled:\n",
    "            labels = self.get_label(row)\n",
    "            return token_ids, seg_ids, labels\n",
    "        else:\n",
    "            return token_ids, seg_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def select_tokens(self, tokens, max_num):\n",
    "        if len(tokens) <= max_num:\n",
    "            return tokens\n",
    "        if self.train_mode:\n",
    "            num_remove = len(tokens) - max_num\n",
    "            remove_start = random.randint(0, len(tokens)-num_remove-1)\n",
    "            return tokens[:remove_start] + tokens[remove_start + num_remove:]\n",
    "        else:\n",
    "            return tokens[:max_num//2] + tokens[-(max_num - max_num//2):]\n",
    "\n",
    "    def trim_input_q(self, title, question, t_max_len=58, q_max_len=442):\n",
    "            \n",
    "        t = self.tokenizer.tokenize(title)\n",
    "        q = self.tokenizer.tokenize(question)\n",
    "\n",
    "        t_len = len(t)\n",
    "        q_len = len(q)\n",
    "\n",
    "        if (t_len+q_len+12) > self.max_sequence_length:\n",
    "\n",
    "            if t_max_len > t_len:\n",
    "                t_new_len = t_len\n",
    "                q_new_len = q_max_len + t_max_len - t_len\n",
    "            else:\n",
    "                t_new_len = t_max_len\n",
    "                q_new_len = q_max_len\n",
    "\n",
    "            if t_new_len+q_new_len+12 != self.max_sequence_length:\n",
    "                raise ValueError(\"New sequence length should be %d, but is %d\" \n",
    "                                 % (self.max_sequence_length, (t_new_len+q_new_len+12)))\n",
    "\n",
    "            t = t[:t_new_len]\n",
    "\n",
    "            q_len_head = round(q_new_len*3/4)\n",
    "            q_len_tail = -1 * (q_new_len - q_len_head)\n",
    "            q = q[:q_len_head] + q[q_len_tail:]\n",
    "\n",
    "        return t, q\n",
    "    \n",
    "    def trim_input_a(self, title, answer, t_max_len=58, a_max_len=442):\n",
    "        \n",
    "        t = self.tokenizer.tokenize(title)\n",
    "        a = self.tokenizer.tokenize(answer)\n",
    "\n",
    "        t_len = len(t)\n",
    "        a_len = len(a)\n",
    "\n",
    "        if (t_len+a_len+12) > self.max_sequence_length:\n",
    "\n",
    "            if t_max_len > t_len:\n",
    "                t_new_len = t_len\n",
    "                a_new_len = a_max_len + t_max_len - t_len\n",
    "            else:\n",
    "                t_new_len = t_max_len\n",
    "                a_new_len = a_max_len\n",
    "\n",
    "            if t_new_len+a_new_len+12 != self.max_sequence_length:\n",
    "                raise ValueError(\"New sequence length should be %d, but is %d\" \n",
    "                                 % (self.max_sequence_length, (t_new_len+a_new_len+12)))\n",
    "\n",
    "            t = t[:t_new_len]\n",
    "            \n",
    "            a_len_head = round(a_new_len*3/4)\n",
    "            a_len_tail = -1 * (a_new_len - a_len_head)\n",
    "            a = a[:a_len_head] + a[a_len_tail:]\n",
    "\n",
    "        return t, a\n",
    "    \n",
    "    def get_token_ids(self, row):\n",
    "        if self.target_level == 0:\n",
    "            t_tokens, q_tokens = self.trim_input_q(row.question_title, row.question_body)\n",
    "            tokens = [TOKEN_CLS] + ['[CATEGORY]'] + ['[category:{}]'.format(row['category'])] + \\\n",
    "                        ['[DOMAIN]'] + ['[domain:{}]'.format(row['host'].split('.')[-2])] + \\\n",
    "                        ['[HOST]'] + [row['host']] + [TOKEN_SEP] + ['[TITLE]'] + [TOKEN_SEP] + \\\n",
    "                        t_tokens + ['[BODY]'] + q_tokens + [TOKEN_SEP]\n",
    "        elif self.target_level == 1:\n",
    "            t_tokens, a_tokens = self.trim_input_a(row.question_title, row.answer)\n",
    "            tokens = [TOKEN_CLS] + ['[CATEGORY]'] + ['[category:{}]'.format(row['category'])] + \\\n",
    "                        ['[DOMAIN]'] + ['[domain:{}]'.format(row['host'].split('.')[-2])] + \\\n",
    "                        ['[HOST]'] + [row['host']] + [TOKEN_SEP] + ['[TITLE]'] + [TOKEN_SEP] + \\\n",
    "                        t_tokens + ['[BODY]'] + a_tokens + [TOKEN_SEP]\n",
    "        else:\n",
    "            raise ValueError('target_level should be 0 or 1')\n",
    "        token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "        if len(token_ids) < self.max_sequence_length:\n",
    "            token_ids += [0] * (self.max_sequence_length - len(token_ids))\n",
    "        ids = torch.tensor(token_ids)\n",
    "        seg_ids = self.get_seg_ids(ids)\n",
    "        return ids, seg_ids\n",
    "    \n",
    "    def get_seg_ids(self, ids):\n",
    "        seg_ids = torch.zeros_like(ids)\n",
    "        seg_idx = 0\n",
    "        first_sep = True\n",
    "        for i, e in enumerate(ids):\n",
    "            seg_ids[i] = seg_idx\n",
    "            if e == SEP_TOKEN_ID:\n",
    "                if first_sep:\n",
    "                    first_sep = False\n",
    "                else:\n",
    "                    seg_idx = 1\n",
    "        pad_idx = torch.nonzero(ids == 0)\n",
    "        seg_ids[pad_idx] = 0\n",
    "        return seg_ids\n",
    "\n",
    "    def get_label(self, row):\n",
    "        return torch.tensor(row[self.target_columns].values.astype(np.float32))\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        token_ids = torch.stack([x[0] for x in batch])\n",
    "        seg_ids = torch.stack([x[1] for x in batch])\n",
    "        if self.labeled:\n",
    "            labels = torch.stack([x[2] for x in batch])\n",
    "            return token_ids, seg_ids, labels\n",
    "        else:\n",
    "            return token_ids, seg_ids\n",
    "\n",
    "\n",
    "def get_test_loader(batch_size=BATCH_SIZE, target_level=0):\n",
    "    df = pd.read_csv(LOCAL_PATH+'/test.csv')\n",
    "    ds_test = QuestDataset(df, None, target_level=target_level, train_mode=False, labeled=False)\n",
    "    loader = torch.utils.data.DataLoader(ds_test, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS, collate_fn=ds_test.collate_fn, drop_last=False)\n",
    "    loader.num = len(df)\n",
    "    return loader\n",
    "\n",
    "\n",
    "def get_train_val_loaders(target_cols, batch_size=BATCH_SIZE, target_level=0, val_batch_size=4, ifold=0):\n",
    "\n",
    "    df = pd.read_csv(LOCAL_PATH+'/train.csv')\n",
    "    df = shuffle(df, random_state=SEED)\n",
    "    gkf = GroupKFold(n_splits=N_SPLIT).split(X=df.question_body, groups=df.question_body)\n",
    "    for fold, (train_idx, valid_idx) in enumerate(gkf):\n",
    "        if fold == ifold:\n",
    "            df_train = df.iloc[train_idx]\n",
    "            df_val = df.iloc[valid_idx]\n",
    "            break\n",
    "\n",
    "    ds_train = QuestDataset(df_train, target_cols, target_level=target_level)\n",
    "    train_loader = torch.utils.data.DataLoader(ds_train, batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS, collate_fn=ds_train.collate_fn, drop_last=True)\n",
    "    train_loader.num = len(df_train)\n",
    "\n",
    "    ds_val = QuestDataset(df_val, target_cols, target_level=target_level, train_mode=False)\n",
    "    val_loader = torch.utils.data.DataLoader(ds_val, batch_size=val_batch_size, shuffle=False, num_workers=NUM_WORKERS, collate_fn=ds_val.collate_fn, drop_last=False)\n",
    "    val_loader.num = len(df_val)\n",
    "    val_loader.df = df_val\n",
    "\n",
    "    return train_loader, val_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Identity\n",
    "import torch.nn.functional as F\n",
    "from transformers import PreTrainedModel\n",
    "from transformers.configuration_utils import PretrainedConfig\n",
    "\n",
    "    \n",
    "T5_PRETRAINED_CONFIG_ARCHIVE_MAP = {\n",
    "    \"t5-small\": \"https://s3.amazonaws.com/models.huggingface.co/bert/t5-small-config.json\",\n",
    "    \"t5-base\": \"https://s3.amazonaws.com/models.huggingface.co/bert/t5-base-config.json\",\n",
    "    \"t5-large\": \"https://s3.amazonaws.com/models.huggingface.co/bert/t5-large-config.json\",\n",
    "    \"t5-3b\": \"https://s3.amazonaws.com/models.huggingface.co/bert/t5-3b-config.json\",\n",
    "    \"t5-11b\": \"https://s3.amazonaws.com/models.huggingface.co/bert/t5-11b-config.json\",\n",
    "}\n",
    "T5_PRETRAINED_MODEL_ARCHIVE_MAP = {\n",
    "    \"t5-small\": \"https://s3.amazonaws.com/models.huggingface.co/bert/t5-small-pytorch_model.bin\",\n",
    "    \"t5-base\": \"https://s3.amazonaws.com/models.huggingface.co/bert/t5-base-pytorch_model.bin\",\n",
    "    \"t5-large\": \"https://s3.amazonaws.com/models.huggingface.co/bert/t5-large-pytorch_model.bin\",\n",
    "    \"t5-3b\": \"https://s3.amazonaws.com/models.huggingface.co/bert/t5-3b-pytorch_model.bin\",\n",
    "    \"t5-11b\": \"https://s3.amazonaws.com/models.huggingface.co/bert/t5-11b-pytorch_model.bin\",\n",
    "}\n",
    "DUMMY_INPUTS = [[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]]\n",
    "DUMMY_MASK = [[1, 1, 1, 1, 1], [1, 1, 1, 0, 0], [0, 0, 0, 1, 1]]\n",
    "\n",
    "class SequenceSummary(nn.Module):\n",
    "    r\"\"\" Compute a single vector summary of a sequence hidden states according to various possibilities:\n",
    "        Args of the config class:\n",
    "            summary_type:\n",
    "                - 'last' => [default] take the last token hidden state (like XLNet)\n",
    "                - 'first' => take the first token hidden state (like Bert)\n",
    "                - 'mean' => take the mean of all tokens hidden states\n",
    "                - 'cls_index' => supply a Tensor of classification token position (GPT/GPT-2)\n",
    "                - 'attn' => Not implemented now, use multi-head attention\n",
    "            summary_use_proj: Add a projection after the vector extraction\n",
    "            summary_proj_to_labels: If True, the projection outputs to config.num_labels classes (otherwise to hidden_size). Default: False.\n",
    "            summary_activation: 'tanh' => add a tanh activation to the output, Other => no activation. Default\n",
    "            summary_first_dropout: Add a dropout before the projection and activation\n",
    "            summary_last_dropout: Add a dropout after the projection and activation\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(SequenceSummary, self).__init__()\n",
    "\n",
    "        self.summary_type = config.summary_type if hasattr(config, 'summary_use_proj') else 'last'\n",
    "        if self.summary_type == 'attn':\n",
    "            # We should use a standard multi-head attention module with absolute positional embedding for that.\n",
    "            # Cf. https://github.com/zihangdai/xlnet/blob/master/modeling.py#L253-L276\n",
    "            # We can probably just use the multi-head attention module of PyTorch >=1.1.0\n",
    "            raise NotImplementedError\n",
    "\n",
    "        self.summary = Identity()\n",
    "        if hasattr(config, 'summary_use_proj') and config.summary_use_proj:\n",
    "            if hasattr(config, 'summary_proj_to_labels') and config.summary_proj_to_labels and config.num_labels > 0:\n",
    "                num_classes = config.num_labels\n",
    "            else:\n",
    "                num_classes = config.hidden_size\n",
    "            self.summary = nn.Linear(config.hidden_size, num_classes)\n",
    "\n",
    "        self.activation = Identity()\n",
    "        if hasattr(config, 'summary_activation') and config.summary_activation == 'tanh':\n",
    "            self.activation = nn.Tanh()\n",
    "\n",
    "        self.first_dropout = Identity()\n",
    "        if hasattr(config, 'summary_first_dropout') and config.summary_first_dropout > 0:\n",
    "            self.first_dropout = nn.Dropout(config.summary_first_dropout)\n",
    "\n",
    "        self.last_dropout = Identity()\n",
    "        if hasattr(config, 'summary_last_dropout') and config.summary_last_dropout > 0:\n",
    "            self.last_dropout = nn.Dropout(config.summary_last_dropout)\n",
    "\n",
    "    def forward(self, hidden_states, cls_index=None):\n",
    "        \"\"\" hidden_states: float Tensor in shape [bsz, seq_len, hidden_size], the hidden-states of the last layer.\n",
    "            cls_index: [optional] position of the classification token if summary_type == 'cls_index',\n",
    "                shape (bsz,) or more generally (bsz, ...) where ... are optional leading dimensions of hidden_states.\n",
    "                if summary_type == 'cls_index' and cls_index is None:\n",
    "                    we take the last token of the sequence as classification token\n",
    "        \"\"\"\n",
    "        if self.summary_type == 'last':\n",
    "            output = hidden_states[:, -1]\n",
    "        elif self.summary_type == 'first':\n",
    "            output = hidden_states[:, 0]\n",
    "        elif self.summary_type == 'mean':\n",
    "            output = hidden_states.mean(dim=1)\n",
    "        elif self.summary_type == 'cls_index':\n",
    "            if cls_index is None:\n",
    "                cls_index = torch.full_like(hidden_states[..., :1, :], hidden_states.shape[-2]-1, dtype=torch.long)\n",
    "            else:\n",
    "                cls_index = cls_index.unsqueeze(-1).unsqueeze(-1)\n",
    "                cls_index = cls_index.expand((-1,) * (cls_index.dim()-1) + (hidden_states.size(-1),))\n",
    "            # shape of cls_index: (bsz, XX, 1, hidden_size) where XX are optional leading dim of hidden_states\n",
    "            output = hidden_states.gather(-2, cls_index).squeeze(-2) # shape (bsz, XX, hidden_size)\n",
    "        elif self.summary_type == 'attn':\n",
    "            raise NotImplementedError\n",
    "\n",
    "        output = self.first_dropout(output)\n",
    "        output = self.summary(output)\n",
    "        output = self.activation(output)\n",
    "        output = self.last_dropout(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "class T5Config(PretrainedConfig):\n",
    "    r\"\"\"\n",
    "        :class:`~transformers.T5Config` is the configuration class to store the configuration of a\n",
    "        `T5Model`.\n",
    "        Arguments:\n",
    "            vocab_size_or_config_json_file: Vocabulary size of `inputs_ids` in `T5Model`.\n",
    "            hidden_size: Size of the encoder layers and the pooler layer.\n",
    "            num_hidden_layers: Number of hidden layers in the Transformer encoder.\n",
    "            num_attention_heads: Number of attention heads for each attention layer in\n",
    "                the Transformer encoder.\n",
    "            intermediate_size: The size of the \"intermediate\" (i.e., feed-forward)\n",
    "                layer in the Transformer encoder.\n",
    "            hidden_act: The non-linear activation function (function or string) in the\n",
    "                encoder and pooler. If string, \"gelu\", \"relu\", \"swish\" and \"gelu_new\" are supported.\n",
    "            hidden_dropout_prob: The dropout probabilitiy for all fully connected\n",
    "                layers in the embeddings, encoder, and pooler.\n",
    "            attention_probs_dropout_prob: The dropout ratio for the attention\n",
    "                probabilities.\n",
    "            max_position_embeddings: The maximum sequence length that this model might\n",
    "                ever be used with. Typically set this to something large just in case\n",
    "                (e.g., 512 or 1024 or 2048).\n",
    "            type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n",
    "                `T5Model`.\n",
    "            initializer_factor: A factor for initializing all weight matrices (should be kept to 1.0, used for initialization testing).\n",
    "            layer_norm_eps: The epsilon used by LayerNorm.\n",
    "    \"\"\"\n",
    "    pretrained_config_archive_map = T5_PRETRAINED_CONFIG_ARCHIVE_MAP\n",
    "    model_type = \"t5\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size=32128,\n",
    "        n_positions=512,\n",
    "        d_model=512,\n",
    "        d_kv=64,\n",
    "        d_ff=2048,\n",
    "        num_layers=6,\n",
    "        num_heads=8,\n",
    "        relative_attention_num_buckets=32,\n",
    "        dropout_rate=0.1,\n",
    "        layer_norm_epsilon=1e-6,\n",
    "        initializer_factor=1.0,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_positions = n_positions\n",
    "        self.d_model = d_model\n",
    "        self.d_kv = d_kv\n",
    "        self.d_ff = d_ff\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.relative_attention_num_buckets = relative_attention_num_buckets\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.layer_norm_epsilon = layer_norm_epsilon\n",
    "        self.initializer_factor = initializer_factor\n",
    "\n",
    "    @property\n",
    "    def max_position_embeddings(self):\n",
    "        return self.n_positions\n",
    "\n",
    "    @property\n",
    "    def hidden_size(self):\n",
    "        return self.d_model\n",
    "\n",
    "    @property\n",
    "    def num_attention_heads(self):\n",
    "        return self.num_heads\n",
    "\n",
    "    @property\n",
    "    def num_hidden_layers(self):\n",
    "        return self.num_layers\n",
    "\n",
    "def add_start_docstrings(*docstr):\n",
    "    def docstring_decorator(fn):\n",
    "        fn.__doc__ = \"\".join(docstr) + (fn.__doc__ if fn.__doc__ is not None else \"\")\n",
    "        return fn\n",
    "\n",
    "    return docstring_decorator\n",
    "\n",
    "def load_tf_weights_in_t5(model, config, tf_checkpoint_path):\n",
    "    \"\"\" Load tf checkpoints in a pytorch model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import re\n",
    "        import numpy as np\n",
    "        import tensorflow as tf\n",
    "    except ImportError:\n",
    "        logger.error(\n",
    "            \"Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see \"\n",
    "            \"https://www.tensorflow.org/install/ for installation instructions.\"\n",
    "        )\n",
    "        raise\n",
    "    tf_path = os.path.abspath(tf_checkpoint_path)\n",
    "    logger.info(\"Converting TensorFlow checkpoint from {}\".format(tf_path))\n",
    "    # Load weights from TF model\n",
    "    init_vars = tf.train.list_variables(tf_path)\n",
    "    names = []\n",
    "    tf_weights = {}\n",
    "    for name, shape in init_vars:\n",
    "        logger.info(\"Loading TF weight {} with shape {}\".format(name, shape))\n",
    "        array = tf.train.load_variable(tf_path, name)\n",
    "        names.append(name)\n",
    "        tf_weights[name] = array\n",
    "\n",
    "    for txt_name in names:\n",
    "        name = txt_name.split(\"/\")\n",
    "        # adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v\n",
    "        # which are not required for using pretrained model\n",
    "        if any(n in [\"adam_v\", \"adam_m\", \"global_step\"] for n in name):\n",
    "            logger.info(\"Skipping {}\".format(\"/\".join(name)))\n",
    "            tf_weights.pop(txt_name, None)\n",
    "            continue\n",
    "        if \"_slot_\" in name[-1]:\n",
    "            logger.info(\"Skipping {}\".format(\"/\".join(name)))\n",
    "            tf_weights.pop(txt_name, None)\n",
    "            continue\n",
    "        pointer = model\n",
    "        array = tf_weights[txt_name]\n",
    "        for m_name in name:\n",
    "            if re.fullmatch(r\"[A-Za-z]+_\\d+\", m_name):\n",
    "                scope_names = re.split(r\"_(\\d+)\", m_name)\n",
    "            else:\n",
    "                scope_names = [m_name]\n",
    "            if scope_names[0] in [\"kernel\", \"scale\", \"embedding\"]:\n",
    "                pointer = getattr(pointer, \"weight\")\n",
    "            # elif scope_names[0] == 'scale':\n",
    "            #     pointer = getattr(pointer, 'weight')\n",
    "            # elif scope_names[0] == 'output_bias' or scope_names[0] == 'beta':\n",
    "            #     pointer = getattr(pointer, 'bias')\n",
    "            # elif scope_names[0] == 'squad':\n",
    "            #     pointer = getattr(pointer, 'classifier')\n",
    "            else:\n",
    "                try:\n",
    "                    pointer = getattr(pointer, scope_names[0])\n",
    "                except AttributeError:\n",
    "                    logger.info(\"Skipping {}\".format(\"/\".join(name)))\n",
    "                    continue\n",
    "            if len(scope_names) >= 2:\n",
    "                num = int(scope_names[1])\n",
    "                pointer = pointer[num]\n",
    "        if scope_names[0] not in [\"kernel\", \"scale\", \"embedding\"]:\n",
    "            pointer = getattr(pointer, \"weight\")\n",
    "        if scope_names[0] != \"embedding\":\n",
    "            logger.info(\"Transposing numpy weight of shape {} for {}\".format(array.shape, name))\n",
    "            array = np.transpose(array)\n",
    "        try:\n",
    "            assert pointer.shape == array.shape\n",
    "        except AssertionError as e:\n",
    "            e.args += (pointer.shape, array.shape)\n",
    "            raise\n",
    "        logger.info(\"Initialize PyTorch weight {}\".format(name))\n",
    "        pointer.data = torch.from_numpy(array.astype(np.float32))\n",
    "        tf_weights.pop(txt_name, None)\n",
    "\n",
    "    logger.info(\"Weights not copied to PyTorch model: {}\".format(\", \".join(tf_weights.keys())))\n",
    "    # logger.info(\"Weights not copied to PyTorch model: {}\".format(', '.join(tf_weights.keys())))\n",
    "    return model\n",
    "\n",
    "class T5LayerNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-6):\n",
    "        \"\"\" Construct a layernorm module in the T5 style\n",
    "            No bias and no substraction of mean.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        variance = x.pow(2).mean(-1, keepdim=True)\n",
    "        x = x / torch.sqrt(variance + self.variance_epsilon)\n",
    "        return self.weight * x\n",
    "\n",
    "\n",
    "class T5DenseReluDense(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.wi = nn.Linear(config.d_model, config.d_ff, bias=False)\n",
    "        self.wo = nn.Linear(config.d_ff, config.d_model, bias=False)\n",
    "        self.dropout = nn.Dropout(config.dropout_rate)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        h = self.wi(hidden_states)\n",
    "        h = F.relu(h)\n",
    "        h = self.dropout(h)\n",
    "        h = self.wo(h)\n",
    "        return h\n",
    "\n",
    "\n",
    "class T5LayerFF(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.DenseReluDense = T5DenseReluDense(config)\n",
    "        self.layer_norm = T5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n",
    "        self.dropout = nn.Dropout(config.dropout_rate)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        norm_x = self.layer_norm(hidden_states)\n",
    "        y = self.DenseReluDense(norm_x)\n",
    "        layer_output = hidden_states + self.dropout(y)\n",
    "        return layer_output\n",
    "\n",
    "\n",
    "class T5Attention(nn.Module):\n",
    "    NEW_ID = itertools.count()\n",
    "\n",
    "    def __init__(self, config, has_relative_attention_bias=False):\n",
    "        super().__init__()\n",
    "        self.layer_id = next(T5Attention.NEW_ID)\n",
    "        self.is_decoder = config.is_decoder\n",
    "        self.has_relative_attention_bias = has_relative_attention_bias\n",
    "\n",
    "        self.output_attentions = config.output_attentions\n",
    "        self.relative_attention_num_buckets = config.relative_attention_num_buckets\n",
    "        self.d_model = config.d_model\n",
    "        self.d_kv = config.d_kv\n",
    "        self.n_heads = config.num_heads\n",
    "        self.dropout = config.dropout_rate\n",
    "        self.inner_dim = self.n_heads * self.d_kv\n",
    "\n",
    "        # Mesh TensorFlow initialization to avoid scaling before softmax\n",
    "        self.q = nn.Linear(self.d_model, self.inner_dim, bias=False)\n",
    "        self.k = nn.Linear(self.d_model, self.inner_dim, bias=False)\n",
    "        self.v = nn.Linear(self.d_model, self.inner_dim, bias=False)\n",
    "        self.o = nn.Linear(self.inner_dim, self.d_model, bias=False)\n",
    "\n",
    "        if self.has_relative_attention_bias:\n",
    "            self.relative_attention_bias = nn.Embedding(self.relative_attention_num_buckets, self.n_heads)\n",
    "        self.pruned_heads = set()\n",
    "\n",
    "    def prune_heads(self, heads):\n",
    "        if len(heads) == 0:\n",
    "            return\n",
    "        mask = torch.ones(self.n_heads, self.d_kv)\n",
    "        heads = set(heads) - self.pruned_heads\n",
    "        for head in heads:\n",
    "            head -= sum(1 if h < head else 0 for h in self.pruned_heads)\n",
    "            mask[head] = 0\n",
    "        mask = mask.view(-1).contiguous().eq(1)\n",
    "        index = torch.arange(len(mask))[mask].long()\n",
    "        # Prune linear layers\n",
    "        self.q = prune_linear_layer(self.q, index)\n",
    "        self.k = prune_linear_layer(self.k, index)\n",
    "        self.v = prune_linear_layer(self.v, index)\n",
    "        self.o = prune_linear_layer(self.o, index, dim=1)\n",
    "        # Update hyper params\n",
    "        self.n_heads = self.n_heads - len(heads)\n",
    "        self.inner_dim = self.d_kv * self.n_heads\n",
    "        self.pruned_heads = self.pruned_heads.union(heads)\n",
    "\n",
    "    @staticmethod\n",
    "    def _relative_position_bucket(relative_position, bidirectional=True, num_buckets=32, max_distance=128):\n",
    "        \"\"\"\n",
    "        Adapted from Mesh Tensorflow:\n",
    "        https://github.com/tensorflow/mesh/blob/0cb87fe07da627bf0b7e60475d59f95ed6b5be3d/mesh_tensorflow/transformer/transformer_layers.py#L593\n",
    "        Translate relative position to a bucket number for relative attention.\n",
    "        The relative position is defined as memory_position - query_position, i.e.\n",
    "        the distance in tokens from the attending position to the attended-to\n",
    "        position.  If bidirectional=False, then positive relative positions are\n",
    "        invalid.\n",
    "        We use smaller buckets for small absolute relative_position and larger buckets\n",
    "        for larger absolute relative_positions.  All relative positions >=max_distance\n",
    "        map to the same bucket.  All relative positions <=-max_distance map to the\n",
    "        same bucket.  This should allow for more graceful generalization to longer\n",
    "        sequences than the model has been trained on.\n",
    "        Args:\n",
    "            relative_position: an int32 Tensor\n",
    "            bidirectional: a boolean - whether the attention is bidirectional\n",
    "            num_buckets: an integer\n",
    "            max_distance: an integer\n",
    "        Returns:\n",
    "            a Tensor with the same shape as relative_position, containing int32\n",
    "            values in the range [0, num_buckets)\n",
    "        \"\"\"\n",
    "        ret = 0\n",
    "        n = -relative_position\n",
    "        if bidirectional:\n",
    "            num_buckets //= 2\n",
    "            ret += (n < 0).to(torch.long) * num_buckets  # mtf.to_int32(mtf.less(n, 0)) * num_buckets\n",
    "            n = torch.abs(n)\n",
    "        else:\n",
    "            n = torch.max(n, torch.zeros_like(n))\n",
    "        # now n is in the range [0, inf)\n",
    "\n",
    "        # half of the buckets are for exact increments in positions\n",
    "        max_exact = num_buckets // 2\n",
    "        is_small = n < max_exact\n",
    "\n",
    "        # The other half of the buckets are for logarithmically bigger bins in positions up to max_distance\n",
    "        val_if_large = max_exact + (\n",
    "            torch.log(n.float() / max_exact) / math.log(max_distance / max_exact) * (num_buckets - max_exact)\n",
    "        ).to(torch.long)\n",
    "        val_if_large = torch.min(val_if_large, torch.full_like(val_if_large, num_buckets - 1))\n",
    "\n",
    "        ret += torch.where(is_small, n, val_if_large)\n",
    "        return ret\n",
    "\n",
    "    def compute_bias(self, qlen, klen):\n",
    "        \"\"\" Compute binned relative position bias \"\"\"\n",
    "        context_position = torch.arange(qlen, dtype=torch.long)[:, None]\n",
    "        memory_position = torch.arange(klen, dtype=torch.long)[None, :]\n",
    "        relative_position = memory_position - context_position  # shape (qlen, klen)\n",
    "        rp_bucket = self._relative_position_bucket(\n",
    "            relative_position,  # shape (qlen, klen)\n",
    "            bidirectional=not self.is_decoder,\n",
    "            num_buckets=self.relative_attention_num_buckets,\n",
    "        )\n",
    "        rp_bucket = rp_bucket.to(self.relative_attention_bias.weight.device)\n",
    "        values = self.relative_attention_bias(rp_bucket)  # shape (qlen, klen, num_heads)\n",
    "        values = values.permute([2, 0, 1]).unsqueeze(0)  # shape (1, num_heads, qlen, klen)\n",
    "        return values\n",
    "\n",
    "    def forward(self, input, mask=None, kv=None, position_bias=None, cache=None, head_mask=None):\n",
    "        \"\"\"\n",
    "        Self-attention (if kv is None) or attention over source sentence (provided by kv).\n",
    "        \"\"\"\n",
    "        # Input is (bs, qlen, dim)\n",
    "        # Mask is (bs, klen) (non-causal) or (bs, klen, klen)\n",
    "        bs, qlen, dim = input.size()\n",
    "        if kv is None:\n",
    "            klen = qlen if cache is None else cache[\"slen\"] + qlen\n",
    "        else:\n",
    "            klen = kv.size(1)\n",
    "\n",
    "        def shape(x):\n",
    "            \"\"\"  projection \"\"\"\n",
    "            return x.view(bs, -1, self.n_heads, self.d_kv).transpose(1, 2)\n",
    "\n",
    "        def unshape(x):\n",
    "            \"\"\"  compute context \"\"\"\n",
    "            return x.transpose(1, 2).contiguous().view(bs, -1, self.inner_dim)\n",
    "\n",
    "        q = shape(self.q(input))  # (bs, n_heads, qlen, dim_per_head)\n",
    "        if kv is None:\n",
    "            k = shape(self.k(input))  # (bs, n_heads, qlen, dim_per_head)\n",
    "            v = shape(self.v(input))  # (bs, n_heads, qlen, dim_per_head)\n",
    "        elif cache is None or self.layer_id not in cache:\n",
    "            k = v = kv\n",
    "            k = shape(self.k(k))  # (bs, n_heads, qlen, dim_per_head)\n",
    "            v = shape(self.v(v))  # (bs, n_heads, qlen, dim_per_head)\n",
    "\n",
    "        if cache is not None:\n",
    "            if self.layer_id in cache:\n",
    "                if kv is None:\n",
    "                    k_, v_ = cache[self.layer_id]\n",
    "                    k = torch.cat([k_, k], dim=2)  # (bs, n_heads, klen, dim_per_head)\n",
    "                    v = torch.cat([v_, v], dim=2)  # (bs, n_heads, klen, dim_per_head)\n",
    "                else:\n",
    "                    k, v = cache[self.layer_id]\n",
    "            cache[self.layer_id] = (k, v)\n",
    "\n",
    "        # q = q / math.sqrt(dim_per_head)                                     # No scaling in T5\n",
    "        scores = torch.einsum(\"bnqd,bnkd->bnqk\", q, k)  # (bs, n_heads, qlen, klen)\n",
    "\n",
    "        if position_bias is None:\n",
    "            if not self.has_relative_attention_bias:\n",
    "                raise ValueError(\"No position_bias provided and no weights to compute position_bias\")\n",
    "            position_bias = self.compute_bias(qlen, klen)\n",
    "            if mask is not None:\n",
    "                position_bias = position_bias + mask  # (bs, n_heads, qlen, klen)\n",
    "\n",
    "        scores += position_bias\n",
    "        weights = F.softmax(scores.float(), dim=-1).type_as(scores)  # (bs, n_heads, qlen, klen)\n",
    "        weights = F.dropout(weights, p=self.dropout, training=self.training)  # (bs, n_heads, qlen, klen)\n",
    "\n",
    "        # Mask heads if we want to\n",
    "        if head_mask is not None:\n",
    "            weights = weights * head_mask\n",
    "\n",
    "        context = torch.matmul(weights, v)  # (bs, n_heads, qlen, dim_per_head)\n",
    "        context = unshape(context)  # (bs, qlen, dim)\n",
    "\n",
    "        context = self.o(context)\n",
    "\n",
    "        outputs = (context,)\n",
    "        if self.output_attentions:\n",
    "            outputs = outputs + (weights,)\n",
    "        if self.has_relative_attention_bias:\n",
    "            outputs = outputs + (position_bias,)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class T5LayerSelfAttention(nn.Module):\n",
    "    def __init__(self, config, has_relative_attention_bias=False):\n",
    "        super().__init__()\n",
    "        self.SelfAttention = T5Attention(config, has_relative_attention_bias=has_relative_attention_bias)\n",
    "        self.layer_norm = T5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n",
    "        self.dropout = nn.Dropout(config.dropout_rate)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None, position_bias=None, head_mask=None):\n",
    "        norm_x = self.layer_norm(hidden_states)\n",
    "        attention_output = self.SelfAttention(\n",
    "            norm_x, mask=attention_mask, position_bias=position_bias, head_mask=head_mask\n",
    "        )\n",
    "        y = attention_output[0]\n",
    "        layer_output = hidden_states + self.dropout(y)\n",
    "        outputs = (layer_output,) + attention_output[1:]  # add attentions if we output them\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class T5LayerCrossAttention(nn.Module):\n",
    "    def __init__(self, config, has_relative_attention_bias=False):\n",
    "        super().__init__()\n",
    "        self.EncDecAttention = T5Attention(config, has_relative_attention_bias=has_relative_attention_bias)\n",
    "        self.layer_norm = T5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n",
    "        self.dropout = nn.Dropout(config.dropout_rate)\n",
    "\n",
    "    def forward(self, hidden_states, kv, attention_mask=None, position_bias=None, head_mask=None):\n",
    "        norm_x = self.layer_norm(hidden_states)\n",
    "        attention_output = self.EncDecAttention(\n",
    "            norm_x, mask=attention_mask, kv=kv, position_bias=position_bias, head_mask=head_mask\n",
    "        )\n",
    "        y = attention_output[0]\n",
    "        layer_output = hidden_states + self.dropout(y)\n",
    "        outputs = (layer_output,) + attention_output[1:]  # add attentions if we output them\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class T5Block(nn.Module):\n",
    "    def __init__(self, config, has_relative_attention_bias=False):\n",
    "        super().__init__()\n",
    "        self.is_decoder = config.is_decoder\n",
    "        self.layer = nn.ModuleList()\n",
    "        self.layer.append(T5LayerSelfAttention(config, has_relative_attention_bias=has_relative_attention_bias))\n",
    "        if self.is_decoder:\n",
    "            self.layer.append(T5LayerCrossAttention(config, has_relative_attention_bias=has_relative_attention_bias))\n",
    "            self.layer.append(T5LayerFF(config))\n",
    "        else:\n",
    "            self.layer.append(T5LayerFF(config))\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        position_bias=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        encoder_decoder_position_bias=None,\n",
    "        head_mask=None,\n",
    "    ):\n",
    "        self_attention_outputs = self.layer[0](\n",
    "            hidden_states, attention_mask=attention_mask, position_bias=position_bias, head_mask=head_mask\n",
    "        )\n",
    "        hidden_states = self_attention_outputs[0]\n",
    "        outputs = self_attention_outputs[1:]  # Keep self-attention outputs and relative position weights\n",
    "\n",
    "        if not self.is_decoder:\n",
    "            hidden_states = self.layer[1](hidden_states)\n",
    "        else:\n",
    "            cross_attention_outputs = self.layer[1](\n",
    "                hidden_states,\n",
    "                kv=encoder_hidden_states,\n",
    "                attention_mask=encoder_attention_mask,\n",
    "                position_bias=encoder_decoder_position_bias,\n",
    "                head_mask=head_mask,\n",
    "            )\n",
    "            hidden_states = cross_attention_outputs[0]\n",
    "            outputs = (\n",
    "                outputs + cross_attention_outputs[1:]\n",
    "            )  # Keep cross-attention outputs and relative position weights\n",
    "            hidden_states = self.layer[2](hidden_states)\n",
    "\n",
    "        outputs = (hidden_states,) + outputs  # add attentions if we output them\n",
    "        return outputs  # hidden-states, (self-attention weights), (self-attention position bias), (cross-attention weights), (cross-attention position bias)\n",
    "\n",
    "\n",
    "class T5PreTrainedModel(PreTrainedModel):\n",
    "    \"\"\" An abstract class to handle weights initialization and\n",
    "        a simple interface for downloading and loading pretrained models.\n",
    "    \"\"\"\n",
    "\n",
    "    config_class = T5Config\n",
    "    pretrained_model_archive_map = T5_PRETRAINED_MODEL_ARCHIVE_MAP\n",
    "    load_tf_weights = load_tf_weights_in_t5\n",
    "    base_model_prefix = \"transformer\"\n",
    "\n",
    "    @property\n",
    "    def dummy_inputs(self):\n",
    "        input_ids = torch.tensor(DUMMY_INPUTS)\n",
    "        input_mask = torch.tensor(DUMMY_MASK)\n",
    "        dummy_inputs = {\n",
    "            \"decoder_input_ids\": input_ids,\n",
    "            \"encoder_input_ids\": input_ids,\n",
    "            \"decoder_attention_mask\": input_mask,\n",
    "        }\n",
    "        return dummy_inputs\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\" Initialize the weights \"\"\"\n",
    "        factor = self.config.initializer_factor  # Used for testing weights initialization\n",
    "        if isinstance(module, T5LayerNorm):\n",
    "            module.weight.data.fill_(factor * 1.0)\n",
    "        elif isinstance(module, (T5Model, T5WithLMHeadModel)):\n",
    "            # Mesh TensorFlow embeddings initialization\n",
    "            # See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L1624\n",
    "            module.shared.weight.data.normal_(mean=0.0, std=factor * 1.0)\n",
    "        elif isinstance(module, T5DenseReluDense):\n",
    "            # Mesh TensorFlow FF initialization\n",
    "            # See https://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/transformer/transformer_layers.py#L56\n",
    "            # and https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L89\n",
    "            module.wi.weight.data.normal_(mean=0.0, std=factor * ((self.config.d_model) ** -0.5))\n",
    "            if hasattr(module.wi, \"bias\") and module.wi.bias is not None:\n",
    "                module.wi.bias.data.zero_()\n",
    "            module.wo.weight.data.normal_(mean=0.0, std=factor * ((self.config.d_ff) ** -0.5))\n",
    "            if hasattr(module.wo, \"bias\") and module.wo.bias is not None:\n",
    "                module.wo.bias.data.zero_()\n",
    "        elif isinstance(module, T5Attention):\n",
    "            # Mesh TensorFlow attention initialization to avoid scaling before softmax\n",
    "            # See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/attention.py#L136\n",
    "            d_model = self.config.d_model\n",
    "            d_kv = self.config.d_kv\n",
    "            n_heads = self.config.num_heads\n",
    "            module.q.weight.data.normal_(mean=0.0, std=factor * ((d_model * d_kv) ** -0.5))\n",
    "            module.k.weight.data.normal_(mean=0.0, std=factor * (d_model ** -0.5))\n",
    "            module.v.weight.data.normal_(mean=0.0, std=factor * (d_model ** -0.5))\n",
    "            module.o.weight.data.normal_(mean=0.0, std=factor * ((n_heads * d_kv) ** -0.5))\n",
    "            if module.has_relative_attention_bias:\n",
    "                module.relative_attention_bias.weight.data.normal_(mean=0.0, std=factor * ((d_model) ** -0.5))\n",
    "\n",
    "\n",
    "class T5Stack(T5PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.output_attentions = config.output_attentions\n",
    "        self.output_hidden_states = config.output_hidden_states\n",
    "        self.is_decoder = config.is_decoder\n",
    "\n",
    "        self.block = nn.ModuleList(\n",
    "            [T5Block(config, has_relative_attention_bias=bool(i == 0)) for i in range(config.num_layers)]\n",
    "        )\n",
    "        self.final_layer_norm = T5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n",
    "        self.dropout = nn.Dropout(config.dropout_rate)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        head_mask=None,\n",
    "    ):\n",
    "\n",
    "        batch_size, seq_length = hidden_states.shape[0], hidden_states.shape[1]\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones(batch_size, seq_length).to(hidden_states.device)\n",
    "        if self.is_decoder and encoder_attention_mask is None:\n",
    "            encoder_seq_length = encoder_hidden_states.shape[1]\n",
    "            encoder_attention_mask = torch.ones(batch_size, encoder_seq_length).to(hidden_states.device)\n",
    "\n",
    "        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n",
    "        # ourselves in which case we just need to make it broadcastable to all heads.\n",
    "        if attention_mask.dim() == 3:\n",
    "            extended_attention_mask = attention_mask[:, None, :, :]\n",
    "        elif attention_mask.dim() == 2:\n",
    "            # Provided a padding mask of dimensions [batch_size, seq_length]\n",
    "            # - if the model is a decoder, apply a causal mask in addition to the padding mask\n",
    "            # - if the model is an encoder, make the mask broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
    "            if self.config.is_decoder:\n",
    "                seq_ids = torch.arange(seq_length, device=hidden_states.device)\n",
    "                causal_mask = seq_ids[None, None, :].repeat(batch_size, seq_length, 1) <= seq_ids[None, :, None]\n",
    "                causal_mask = causal_mask.to(attention_mask)\n",
    "                extended_attention_mask = causal_mask[:, None, :, :] * attention_mask[:, None, None, :]\n",
    "            else:\n",
    "                extended_attention_mask = attention_mask[:, None, None, :]\n",
    "\n",
    "        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
    "        # masked positions, this operation will create a tensor which is 0.0 for\n",
    "        # positions we want to attend and -1e9 for masked positions.\n",
    "        # Since we are adding it to the raw scores before the softmax, this is\n",
    "        # effectively the same as removing these entirely.\n",
    "\n",
    "        # T5 has a mask that can compare sequence ids, we can simulate this here with this transposition\n",
    "        # Cf. https://github.com/tensorflow/mesh/blob/8d2465e9bc93129b913b5ccc6a59aa97abd96ec6/mesh_tensorflow/transformer/transformer_layers.py#L270\n",
    "        # extended_attention_mask = (extended_attention_mask == extended_attention_mask.transpose(-1, -2))\n",
    "\n",
    "        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype)  # fp16 compatibility\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -1e9\n",
    "\n",
    "        if self.is_decoder:\n",
    "            # If a 2D ou 3D attention mask is provided for the cross-attention\n",
    "            # we need to make broadcastabe to [batch_size, num_heads, seq_length, seq_length]\n",
    "            if encoder_attention_mask.dim() == 3:\n",
    "                encoder_extended_attention_mask = encoder_attention_mask[:, None, :, :]\n",
    "            if encoder_attention_mask.dim() == 2:\n",
    "                encoder_extended_attention_mask = encoder_attention_mask[:, None, None, :]\n",
    "\n",
    "            # T5 has a mask that can compare sequence ids, we can simulate this here with this transposition\n",
    "            # Cf. https://github.com/tensorflow/mesh/blob/8d2465e9bc93129b913b5ccc6a59aa97abd96ec6/mesh_tensorflow/transformer/transformer_layers.py#L270\n",
    "            # encoder_extended_attention_mask = (encoder_extended_attention_mask == encoder_extended_attention_mask.transpose(-1, -2))\n",
    "\n",
    "            encoder_extended_attention_mask = encoder_extended_attention_mask.to(\n",
    "                dtype=next(self.parameters()).dtype\n",
    "            )  # fp16 compatibility\n",
    "            encoder_extended_attention_mask = (1.0 - encoder_extended_attention_mask) * -1e9\n",
    "        else:\n",
    "            encoder_extended_attention_mask = None\n",
    "\n",
    "        # Prepare head mask if needed\n",
    "        # 1.0 in head_mask indicate we keep the head\n",
    "        # attention_probs has shape bsz x n_heads x N x N\n",
    "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n",
    "        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
    "        if head_mask is not None:\n",
    "            if head_mask.dim() == 1:\n",
    "                head_mask = head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n",
    "                head_mask = head_mask.expand(self.config.num_layers, -1, -1, -1, -1)\n",
    "            elif head_mask.dim() == 2:\n",
    "                head_mask = (\n",
    "                    head_mask.unsqueeze(1).unsqueeze(-1).unsqueeze(-1)\n",
    "                )  # We can specify head_mask for each layer\n",
    "            head_mask = head_mask.to(\n",
    "                dtype=next(self.parameters()).dtype\n",
    "            )  # switch to fload if need + fp16 compatibility\n",
    "        else:\n",
    "            head_mask = [None] * self.config.num_layers\n",
    "\n",
    "        all_hidden_states = ()\n",
    "        all_attentions = ()\n",
    "        position_bias = None\n",
    "        encoder_decoder_position_bias = None\n",
    "\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        for i, layer_module in enumerate(self.block):\n",
    "            if self.output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            layer_outputs = layer_module(\n",
    "                hidden_states,\n",
    "                attention_mask=extended_attention_mask,\n",
    "                position_bias=position_bias,\n",
    "                encoder_hidden_states=encoder_hidden_states,\n",
    "                encoder_attention_mask=encoder_extended_attention_mask,\n",
    "                encoder_decoder_position_bias=encoder_decoder_position_bias,\n",
    "                head_mask=head_mask[i],\n",
    "            )\n",
    "            # layer_outputs is a tuple with:\n",
    "            # hidden-states, (self-attention weights), (self-attention position bias), (cross-attention weights), (cross-attention position bias)\n",
    "            hidden_states = layer_outputs[0]\n",
    "            if i == 0:\n",
    "                # We share the position biases between the layers - the first layer store them\n",
    "                # layer_outputs = hidden-states, (self-attention weights), (self-attention position bias), (cross-attention weights), (cross-attention position bias)\n",
    "                position_bias = layer_outputs[2 if self.output_attentions else 1]\n",
    "                if self.is_decoder:\n",
    "                    encoder_decoder_position_bias = layer_outputs[4 if self.output_attentions else 2]\n",
    "\n",
    "            if self.output_attentions:\n",
    "                all_attentions = all_attentions + (layer_outputs[1],)  # We keep only self-attention weights for now\n",
    "\n",
    "        hidden_states = self.final_layer_norm(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "\n",
    "        # Add last layer\n",
    "        if self.output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        outputs = (hidden_states,)\n",
    "        if self.output_hidden_states:\n",
    "            outputs = outputs + (all_hidden_states,)\n",
    "        if self.output_attentions:\n",
    "            outputs = outputs + (all_attentions,)\n",
    "        return outputs  # last-layer hidden state, (all hidden states), (all attentions)\n",
    "\n",
    "\n",
    "T5_START_DOCSTRING = r\"\"\"    The T5 model was proposed in\n",
    "    `Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer`_\n",
    "    by Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu.\n",
    "    It's an encoder decoder transformer pre-trained in a text-to-text denoising generative setting.\n",
    "    This model is a PyTorch `torch.nn.Module`_ sub-class. Use it as a regular PyTorch Module and\n",
    "    refer to the PyTorch documentation for all matter related to general usage and behavior.\n",
    "    .. _`Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer`:\n",
    "        https://arxiv.org/abs/1910.10683\n",
    "    .. _`torch.nn.Module`:\n",
    "        https://pytorch.org/docs/stable/nn.html#module\n",
    "    Parameters:\n",
    "        config (:class:`~transformers.T5Config`): Model configuration class with all the parameters of the model.\n",
    "            Initializing with a config file does not load the weights associated with the model, only the configuration.\n",
    "            Check out the :meth:`~transformers.PreTrainedModel.from_pretrained` method to load the model weights.\n",
    "\"\"\"\n",
    "\n",
    "T5_INPUTS_DOCSTRING = r\"\"\"\n",
    "    Inputs:\n",
    "        **input_ids**: ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n",
    "            Indices of input sequence tokens in the vocabulary.\n",
    "            To match pre-training, T5 input sequence should be formatted with [CLS] and [SEP] tokens as follows:\n",
    "            (a) For sequence pairs:\n",
    "                ``tokens:         [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]``\n",
    "            (b) For single sequences:\n",
    "                ``tokens:         [CLS] the dog is hairy . [SEP]``\n",
    "            T5 is a model with relative position embeddings so you should be able to pad the inputs on\n",
    "            the right or the left.\n",
    "            Indices can be obtained using :class:`transformers.T5Tokenizer`.\n",
    "            See :func:`transformers.PreTrainedTokenizer.encode` and\n",
    "            :func:`transformers.PreTrainedTokenizer.convert_tokens_to_ids` for details.\n",
    "        **attention_mask**: (`optional`) ``torch.FloatTensor`` of shape ``(batch_size, sequence_length)``:\n",
    "            Mask to avoid performing attention on padding token indices.\n",
    "            Mask values selected in ``[0, 1]``:\n",
    "            ``1`` for tokens that are NOT MASKED, ``0`` for MASKED tokens.\n",
    "        **head_mask**: (`optional`) ``torch.FloatTensor`` of shape ``(num_heads,)`` or ``(num_layers, num_heads)``:\n",
    "            Mask to nullify selected heads of the self-attention modules.\n",
    "            Mask values selected in ``[0, 1]``:\n",
    "            ``1`` indicates the head is **not masked**, ``0`` indicates the head is **masked**.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@add_start_docstrings(\n",
    "    \"The bare T5 Model transformer outputting raw hidden-states\" \"without any specific head on top.\",\n",
    "    T5_START_DOCSTRING,\n",
    "    T5_INPUTS_DOCSTRING,\n",
    ")\n",
    "class T5Model(T5PreTrainedModel):\n",
    "    r\"\"\"\n",
    "    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n",
    "        **last_hidden_state**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, hidden_size)``\n",
    "            Sequence of hidden-states at the output of the last layer of the model.\n",
    "        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n",
    "            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n",
    "            of shape ``(batch_size, sequence_length, hidden_size)``:\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n",
    "            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n",
    "    Examples::\n",
    "        tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "        model = T5Model.from_pretrained('t5-small')\n",
    "        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\n",
    "        outputs = model(input_ids=input_ids)\n",
    "        last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.shared = nn.Embedding(config.vocab_size, config.d_model)\n",
    "\n",
    "        encoder_config = copy.deepcopy(config)\n",
    "        self.encoder = T5Stack(encoder_config)\n",
    "\n",
    "        decoder_config = copy.deepcopy(config)\n",
    "        decoder_config.is_decoder = True\n",
    "        self.decoder = T5Stack(decoder_config)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.shared\n",
    "\n",
    "    def set_input_embeddings(self, new_embeddings):\n",
    "        self.shared = new_embeddings\n",
    "\n",
    "    def _prune_heads(self, heads_to_prune):\n",
    "        \"\"\" Prunes heads of the model.\n",
    "            heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\n",
    "            See base class PreTrainedModel\n",
    "        \"\"\"\n",
    "        for layer, heads in heads_to_prune.items():\n",
    "            self.encoder.layer[layer].attention.prune_heads(heads)\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        # keyword arguments come in 3 flavors: encoder-specific (prefixed by\n",
    "        # `encoder_`), decoder-specific (prefixed by `decoder_`) and those\n",
    "        # that apply to the model as whole.\n",
    "        # We let the specific kwargs override the common ones in case of conflict.\n",
    "        kwargs_common = dict(\n",
    "            (k, v) for k, v in kwargs.items() if not k.startswith(\"encoder_\") and not k.startswith(\"decoder_\")\n",
    "        )\n",
    "        kwargs_encoder = kwargs_common.copy()\n",
    "        kwargs_decoder = kwargs_common.copy()\n",
    "        kwargs_encoder.update(dict((k[len(\"encoder_\") :], v) for k, v in kwargs.items() if k.startswith(\"encoder_\")))\n",
    "        kwargs_decoder.update(dict((k[len(\"decoder_\") :], v) for k, v in kwargs.items() if k.startswith(\"decoder_\")))\n",
    "\n",
    "        # Encode if needed (training, first prediction pass)\n",
    "        encoder_hidden_states = kwargs_encoder.pop(\"hidden_states\", None)\n",
    "        encoder_attention_mask = kwargs_encoder.get(\"attention_mask\", None)\n",
    "        if encoder_hidden_states is None:\n",
    "            # Convert encoder inputs in embeddings if needed\n",
    "            hidden_states = kwargs_encoder.pop(\"inputs_embeds\", None)\n",
    "            if hidden_states is None:\n",
    "                encoder_inputs_ids = kwargs_encoder.pop(\"input_ids\")\n",
    "                hidden_states = self.shared(encoder_inputs_ids)  # Convert inputs in embeddings\n",
    "\n",
    "            if encoder_attention_mask is not None:\n",
    "                # Apply masking\n",
    "                encoder_attention_mask = (encoder_attention_mask != 0).to(hidden_states)\n",
    "                hidden_states = hidden_states * encoder_attention_mask.unsqueeze(-1)\n",
    "\n",
    "            encoder_outputs = self.encoder(hidden_states, **kwargs_encoder)\n",
    "            encoder_hidden_states = encoder_outputs[0]\n",
    "        else:\n",
    "            encoder_outputs = ()\n",
    "\n",
    "        # Decode\n",
    "        # Convert decoder inputs in embeddings if needed\n",
    "        hidden_states = kwargs_decoder.pop(\"inputs_embeds\", None)\n",
    "        if hidden_states is None:\n",
    "            decoder_inputs_ids = kwargs_decoder.pop(\"input_ids\")\n",
    "            hidden_states = self.shared(decoder_inputs_ids)\n",
    "\n",
    "        kwargs_decoder[\"encoder_hidden_states\"] = encoder_hidden_states\n",
    "        kwargs_decoder[\"encoder_attention_mask\"] = encoder_attention_mask\n",
    "        decoder_outputs = self.decoder(hidden_states, **kwargs_decoder)\n",
    "\n",
    "        return decoder_outputs + encoder_outputs\n",
    "\n",
    "\n",
    "@add_start_docstrings(\"\"\"T5 Model with a `language modeling` head on top. \"\"\", T5_START_DOCSTRING, T5_INPUTS_DOCSTRING)\n",
    "class T5WithLMHeadModel(T5PreTrainedModel):\n",
    "    r\"\"\"\n",
    "        **lm_labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n",
    "            Labels for computing the masked language modeling loss.\n",
    "            Indices should either be in ``[0, ..., config.vocab_size]`` or -100 (see ``input_ids`` docstring).\n",
    "            Tokens with indices set to ``-100`` are ignored (masked), the loss is only computed for the tokens with labels\n",
    "            in ``[0, ..., config.vocab_size]``.\n",
    "    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n",
    "        **loss**: (`optional`, returned when ``lm_labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n",
    "            Masked language modeling loss.\n",
    "        **prediction_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, config.vocab_size)``\n",
    "            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n",
    "        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n",
    "            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n",
    "            of shape ``(batch_size, sequence_length, hidden_size)``:\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n",
    "            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n",
    "    Examples::\n",
    "        tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "        model = T5WithLMHeadModel.from_pretrained('t5-small')\n",
    "        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\n",
    "        outputs = model(input_ids=input_ids, lm_labels=input_ids)\n",
    "        loss, prediction_scores = outputs[:2]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.model_dim = config.d_model\n",
    "\n",
    "        self.shared = nn.Embedding(config.vocab_size, config.d_model)\n",
    "\n",
    "        encoder_config = copy.deepcopy(config)\n",
    "        self.encoder = T5Stack(encoder_config)\n",
    "\n",
    "        decoder_config = copy.deepcopy(config)\n",
    "        decoder_config.is_decoder = True\n",
    "        self.decoder = T5Stack(decoder_config)\n",
    "\n",
    "        self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.shared\n",
    "\n",
    "    def set_input_embeddings(self, new_embeddings):\n",
    "        self.shared = new_embeddings\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.lm_head\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        # keyword arguments come in 3 flavors: encoder-specific (prefixed by\n",
    "        # `encoder_`), decoder-specific (prefixed by `decoder_`) and those\n",
    "        # that apply to the model as whole.\n",
    "        # We let the specific kwargs override the common ones in case of conflict.\n",
    "\n",
    "        lm_labels = kwargs.pop(\"decoder_lm_labels\", None)\n",
    "\n",
    "        kwargs_common = dict(\n",
    "            (k, v) for k, v in kwargs.items() if not k.startswith(\"encoder_\") and not k.startswith(\"decoder_\")\n",
    "        )\n",
    "        kwargs_encoder = kwargs_common.copy()\n",
    "        kwargs_decoder = kwargs_common.copy()\n",
    "        kwargs_encoder.update(dict((k[len(\"encoder_\") :], v) for k, v in kwargs.items() if k.startswith(\"encoder_\")))\n",
    "        kwargs_decoder.update(dict((k[len(\"decoder_\") :], v) for k, v in kwargs.items() if k.startswith(\"decoder_\")))\n",
    "\n",
    "        # Encode if needed (training, first prediction pass)\n",
    "        encoder_hidden_states = kwargs_encoder.pop(\"hidden_states\", None)\n",
    "        if encoder_hidden_states is None:\n",
    "            # Convert encoder inputs in embeddings if needed\n",
    "            hidden_states = kwargs_encoder.pop(\"inputs_embeds\", None)\n",
    "            if hidden_states is None:\n",
    "                encoder_inputs_ids = kwargs_encoder.pop(\"input_ids\")\n",
    "                hidden_states = self.shared(encoder_inputs_ids)  # Convert inputs in embeddings\n",
    "\n",
    "            encoder_outputs = self.encoder(hidden_states, **kwargs_encoder)\n",
    "            encoder_hidden_states = encoder_outputs[0]\n",
    "        else:\n",
    "            encoder_outputs = ()\n",
    "\n",
    "        # Decode\n",
    "        # Convert decoder inputs in embeddings if needed\n",
    "        hidden_states = kwargs_decoder.pop(\"inputs_embeds\", None)\n",
    "        if hidden_states is None:\n",
    "            decoder_inputs_ids = kwargs_decoder.pop(\"input_ids\")\n",
    "            hidden_states = self.shared(decoder_inputs_ids)\n",
    "\n",
    "        kwargs_decoder[\"encoder_hidden_states\"] = encoder_hidden_states\n",
    "        kwargs_decoder[\"encoder_attention_mask\"] = kwargs_encoder.get(\"attention_mask\", None)\n",
    "        decoder_outputs = self.decoder(hidden_states, **kwargs_decoder)\n",
    "\n",
    "        sequence_output = decoder_outputs[0]\n",
    "        # Rescale output before projecting on vocab\n",
    "        # See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/transformer.py#L586\n",
    "        sequence_output = sequence_output * (self.model_dim ** -0.5)\n",
    "        lm_logits = self.lm_head(sequence_output)\n",
    "\n",
    "        decoder_outputs = (lm_logits,) + decoder_outputs[1:]  # Add hidden states and attention if they are here\n",
    "        if lm_labels is not None:\n",
    "            shift_logits = lm_logits[..., :-1, :].contiguous()\n",
    "            shift_labels = lm_labels[..., 1:].contiguous()\n",
    "            loss_fct = CrossEntropyLoss(ignore_index=-100)\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "            decoder_outputs = (\n",
    "                loss,\n",
    "            ) + decoder_outputs  # TODO(thom): Add z_loss https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L666\n",
    "\n",
    "        return decoder_outputs + encoder_outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5ForGoogleQuest(nn.Module):\n",
    "    def __init__(self, n_classes=30):\n",
    "        super(T5ForGoogleQuest, self).__init__()\n",
    "        self.model_name = 'T5ForGoogleQuest'\n",
    "        self.config = T5Config.from_json_file(\n",
    "            MODEL_PRETRAINED_WEIGHTS_PATH + '/t5-base-config.json'\n",
    "        )\n",
    "        self.transformer_model = T5Model.from_pretrained(\n",
    "            MODEL_PRETRAINED_WEIGHTS_PATH + '/t5-base-pytorch_model.bin', \n",
    "            config=self.config\n",
    "        )\n",
    "        self.transformer_model.resize_token_embeddings(len(tokenizer))\n",
    "        self.sequence_summary = SequenceSummary(\n",
    "            config=self.config\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(p = 0.2),\n",
    "            nn.Linear(768, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, ids):\n",
    "        attention_mask = (ids > 0).float()\n",
    "        seq_out = self.transformer_model(\n",
    "            input_ids=ids, attention_mask=attention_mask)[0]\n",
    "        pool_out = self.sequence_summary(seq_out)\n",
    "        logits = self.fc(pool_out)\n",
    "        return logits\n",
    "    \n",
    "# def test_model():\n",
    "#     x = torch.tensor([[1,2,3,4,5, 0, 0], [1,2,3,4,5, 0, 0]])\n",
    "#     model = T5ForGoogleQuest()\n",
    "#     y = model(x)\n",
    "#     print(y)\n",
    "    \n",
    "# test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Training Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "# Training Utility Functions\n",
    "##########################\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "def training_with_accumulation(model, train_loader, optimizer, criterion, scheduler):\n",
    "    \n",
    "    model.train()\n",
    "    avg_loss = 0.\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    bar = tqdm.tqdm_notebook(\n",
    "        enumerate(train_loader), \n",
    "        total=len(train_loader), \n",
    "        postfix={\"train_loss\":0.0,}\n",
    "    )\n",
    "    for idx, batch in bar:\n",
    "        \n",
    "        token_ids, _, labels = batch\n",
    "        token_ids, labels = token_ids.to(DEVICE), labels.to(DEVICE)\n",
    "        \n",
    "        logits = model(token_ids.long())\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        if (idx + 1) % BATCH_ACCUMULATION_COUNT == 0:    \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        avg_loss += loss.item() / (len(train_loader))\n",
    "        \n",
    "        bar.set_postfix(ordered_dict={\n",
    "            \"train_loss\":loss.item(),\n",
    "        })\n",
    "        del token_ids, labels\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def validate_model(model, val_loader, target_cols, batch_size=4, verbose=False):\n",
    "\n",
    "    avg_val_loss = 0.\n",
    "    model.eval()\n",
    "    \n",
    "    y_preds = np.zeros((val_loader.num, len(target_cols)))\n",
    "    y_true = np.zeros((val_loader.num, len(target_cols)))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for idx, batch in enumerate(val_loader):\n",
    "            token_ids, _, labels = batch\n",
    "            token_ids, labels = token_ids.to(DEVICE), labels.to(DEVICE)\n",
    "            \n",
    "            logits = model(token_ids.long())\n",
    "            logits = torch.sigmoid(logits)\n",
    "            \n",
    "            avg_val_loss += criterion(logits, labels).item() / len(val_loader)\n",
    "            y_preds[idx*batch_size : (idx+1)*batch_size] = logits.detach().cpu().squeeze().numpy()\n",
    "            y_true[idx*batch_size : (idx+1)*batch_size]  = labels.detach().cpu().squeeze().numpy()\n",
    "            \n",
    "            del token_ids, labels\n",
    "            \n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        score = 0\n",
    "        for i in range(len(target_cols)):\n",
    "            spear = np.nan_to_num(spearmanr(y_true[:, i], y_preds[:, i]).correlation)\n",
    "            score += spear\n",
    "            if verbose:\n",
    "                print('Target Column {} : {}'.format(target_cols[i], spear))\n",
    "            \n",
    "    return avg_val_loss, score/len(target_cols)\n",
    "\n",
    "\n",
    "def predict(model, test_loader, target_cols, batch_size=BATCH_SIZE):\n",
    "    \n",
    "    test_preds = np.zeros((test_loader.num, len(target_cols)))\n",
    "    \n",
    "    model.eval()\n",
    "    tk0 = tqdm.tqdm_notebook(enumerate(test_loader))\n",
    "    for idx, x_batch in tk0:\n",
    "        with torch.no_grad():\n",
    "            token_ids, _ = x_batch\n",
    "            token_ids = token_ids.to(DEVICE)\n",
    "            predictions = model(token_ids.long())\n",
    "            predictions = torch.sigmoid(predictions)\n",
    "            test_preds[idx*batch_size : (idx+1)*batch_size] = predictions.detach().cpu().squeeze().numpy()\n",
    "\n",
    "    return test_preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Fitting Starts Here\n",
    "Inspired by Nirjhar's [kernel](https://www.kaggle.com/phoenix9032/pytorch-bert-plain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Question Related Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "# Question Related Targets\n",
    "##########################\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "target_cols = output_categories_question\n",
    "target_string = 'questions'\n",
    "target_level = 0\n",
    "\n",
    "if TRAINING:\n",
    "    \n",
    "    gkf = GroupKFold(n_splits=N_SPLIT).split(\n",
    "        X=train.question_body, groups=train.question_body\n",
    "    )\n",
    "    for fold in range(N_SPLIT):\n",
    "\n",
    "        if fold not in FOLD_ID:\n",
    "            continue\n",
    "\n",
    "        train_loader, val_loader = get_train_val_loaders(target_cols=target_cols, target_level=target_level, ifold=fold)\n",
    "\n",
    "        model = T5ForGoogleQuest(n_classes=len(target_cols))\n",
    "        model.zero_grad()\n",
    "        model.to(DEVICE)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if EPOCH_RELEASE > 0:\n",
    "            for param in model.transformer_model.parameters():\n",
    "                param.requires_grad = False\n",
    "            for i, param in enumerate(model.transformer_model.shared.parameters()):\n",
    "                if i >= len(tokenizer)-num_added_tokens:\n",
    "                    param.requires_grad = True\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.9},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=LR, eps=4e-5)\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        scheduler = transformers.get_cosine_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=0.05, \n",
    "            num_training_steps= EPOCHS*len(train_loader)//BATCH_ACCUMULATION_COUNT)\n",
    "\n",
    "        train_start_time = datetime.datetime.now()\n",
    "        best_score = 0.0\n",
    "        reduce_lr_count = 0\n",
    "        for epoch in range(EPOCHS):\n",
    "            epoch_start_time = datetime.datetime.now()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            if epoch == EPOCH_RELEASE:\n",
    "                for param in model.transformer_model.parameters():\n",
    "                    param.requires_grad = True\n",
    "\n",
    "            avg_loss = training_with_accumulation(\n",
    "                model, train_loader, optimizer, criterion, scheduler)\n",
    "            avg_val_loss, val_spearmanr = validate_model(\n",
    "                model, val_loader, target_cols=target_cols, batch_size=4)\n",
    "\n",
    "            print(\"Epoch {} : {} seconds : train loss {:.4f} : valid loss {:.4f} : valid spearmanr {:.4f}\".format(\n",
    "                epoch, (datetime.datetime.now() - epoch_start_time).seconds, avg_loss, avg_val_loss, val_spearmanr))\n",
    "\n",
    "            if val_spearmanr > best_score:\n",
    "                best_score = val_spearmanr\n",
    "                torch.save(model.state_dict(), os.path.join(WEIGHT_PATH, \"model_{}_{}_{}.ckpt\".format(VERSION, target_string, fold)))\n",
    "                early_stopping_count = 0\n",
    "            else:\n",
    "                early_stopping_count += 1\n",
    "                if early_stopping_count == EARLY_STOPPING:\n",
    "                    print(\"Early Stopping : \", epoch)\n",
    "                    break\n",
    "\n",
    "        print('-'*20)\n",
    "        print(\"Fold {} : Total Training Time {}, Best Score : {}\".format(\n",
    "            fold, datetime.datetime.now()-train_start_time, best_score))\n",
    "        print('-'*20)\n",
    "        \n",
    "        model.load_state_dict(torch.load(os.path.join(WEIGHT_PATH, \"model_{}_{}_{}.ckpt\".format(VERSION, target_string, fold))))\n",
    "        avg_val_loss, val_spearmanr = validate_model(\n",
    "            model, val_loader, target_cols=target_cols, batch_size=4, verbose=True)\n",
    "        best_scores.append(val_spearmanr)\n",
    "        \n",
    "        del model\n",
    "        gc.collect()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Answer Related Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "# Answer Related Targets\n",
    "##########################\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "target_cols = output_categories_answer\n",
    "target_string = 'answers'\n",
    "target_level = 1\n",
    "\n",
    "if TRAINING:\n",
    "    \n",
    "    gkf = GroupKFold(n_splits=N_SPLIT).split(\n",
    "        X=train.question_body, groups=train.question_body\n",
    "    )\n",
    "    for fold in range(N_SPLIT):\n",
    "\n",
    "        if fold not in FOLD_ID:\n",
    "            continue\n",
    "\n",
    "        train_loader, val_loader = get_train_val_loaders(target_cols=target_cols, target_level=target_level, ifold=fold)\n",
    "\n",
    "        model = T5ForGoogleQuest(n_classes=len(target_cols))\n",
    "        model.zero_grad()\n",
    "        model.to(DEVICE)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if EPOCH_RELEASE > 0:\n",
    "            for param in model.transformer_model.parameters():\n",
    "                param.requires_grad = False\n",
    "            for i, param in enumerate(model.transformer_model.shared.parameters()):\n",
    "                if i >= len(tokenizer)-num_added_tokens:\n",
    "                    param.requires_grad = True\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.9},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=LR, eps=4e-5)\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        scheduler = transformers.get_cosine_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=0.05, \n",
    "            num_training_steps= EPOCHS*len(train_loader)//BATCH_ACCUMULATION_COUNT)\n",
    "\n",
    "        train_start_time = datetime.datetime.now()\n",
    "        best_score = 0.0\n",
    "        early_stopping_count = 0\n",
    "        for epoch in range(EPOCHS):\n",
    "            epoch_start_time = datetime.datetime.now()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            if epoch == EPOCH_RELEASE:\n",
    "                for param in model.transformer_model.parameters():\n",
    "                    param.requires_grad = True\n",
    "\n",
    "            avg_loss = training_with_accumulation(\n",
    "                model, train_loader, optimizer, criterion, scheduler)\n",
    "            avg_val_loss, val_spearmanr = validate_model(\n",
    "                model, val_loader, target_cols=target_cols, batch_size=4)\n",
    "\n",
    "            print(\"Epoch {} : {} seconds : train loss {:.4f} : valid loss {:.4f} : valid spearmanr {:.4f}\".format(\n",
    "                epoch, (datetime.datetime.now() - epoch_start_time).seconds, avg_loss, avg_val_loss, val_spearmanr))\n",
    "\n",
    "            if val_spearmanr > best_score:\n",
    "                best_score = val_spearmanr\n",
    "                torch.save(model.state_dict(), os.path.join(WEIGHT_PATH, \"model_{}_{}_{}.ckpt\".format(VERSION, target_string, fold)))\n",
    "                early_stopping_count = 0\n",
    "            else:\n",
    "                early_stopping_count += 1\n",
    "                if early_stopping_count == EARLY_STOPPING:\n",
    "                    print(\"Early Stopping : \", epoch)\n",
    "                    break\n",
    "\n",
    "        print('-'*20)\n",
    "        print(\"Fold {} : Total Training Time {}, Best Score : {}\".format(\n",
    "            fold, datetime.datetime.now()-train_start_time, best_score))\n",
    "        print('-'*20)\n",
    "        \n",
    "        model.load_state_dict(torch.load(os.path.join(WEIGHT_PATH, \"model_{}_{}_{}.ckpt\".format(VERSION, target_string, fold))))\n",
    "        avg_val_loss, val_spearmanr = validate_model(\n",
    "            model, val_loader, target_cols=target_cols, batch_size=4, verbose=True)\n",
    "        best_scores.append(val_spearmanr)\n",
    "        \n",
    "        del model\n",
    "        gc.collect()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAINING:\n",
    "    print('Fold {}: Cross Validation Spearman Correlation Coefficient : {}'.format(\n",
    "        FOLD_ID, np.average(\n",
    "            best_scores, \n",
    "            weights=[len(output_categories_question)/(30*len(FOLD_ID))]*len(FOLD_ID)+[len(output_categories_answer)/(30*len(FOLD_ID))]*len(FOLD_ID)\n",
    "        )\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "# Inference Question\n",
    "##########################\n",
    "\n",
    "test_loader = get_test_loader(target_level=0)\n",
    "\n",
    "y_preds_question = np.zeros((test_loader.num, len(output_categories_question)))\n",
    "for fold in range(N_SPLIT):\n",
    "    \n",
    "    if fold not in FOLD_ID:\n",
    "        continue\n",
    "        \n",
    "    model = T5ForGoogleQuest(n_classes=len(output_categories_question))\n",
    "    model.to(DEVICE)\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    model.load_state_dict(torch.load(os.path.join(WEIGHT_PATH, \"model_{}_questions_{}.ckpt\".format(VERSION, fold))))\n",
    "    y_preds_question += predict(model, test_loader, output_categories_question) / len(FOLD_ID)\n",
    "    \n",
    "##########################\n",
    "# Inference Answer\n",
    "##########################\n",
    "\n",
    "test_loader = get_test_loader(target_level=1)\n",
    "\n",
    "y_preds_answer = np.zeros((test_loader.num, len(output_categories_answer)))\n",
    "for fold in range(N_SPLIT):\n",
    "    \n",
    "    if fold not in FOLD_ID:\n",
    "        continue\n",
    "        \n",
    "    model = T5ForGoogleQuest(n_classes=len(output_categories_answer))\n",
    "    model.to(DEVICE)\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    model.load_state_dict(torch.load(os.path.join(WEIGHT_PATH, \"model_{}_answers_{}.ckpt\".format(VERSION, fold))))\n",
    "    y_preds_answer += predict(model, test_loader, output_categories_answer) / len(FOLD_ID)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "# Initial Submission DataFrame\n",
    "##########################\n",
    "\n",
    "y_preds = np.concatenate([y_preds_question, y_preds_answer], axis=1)\n",
    "\n",
    "submission = pd.read_csv(LOCAL_PATH+'/sample_submission.csv')\n",
    "submission.loc[:, 'question_asker_intent_understanding':] = y_preds\n",
    "submission.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "# Postprocessing for question_type_spelling\n",
    "##########################\n",
    "\n",
    "vocab_list_large = [\n",
    "    'pronounced', 'pronounce', 'pronunciation', 'correct adjective', 'How many syllables', 'spell'\n",
    "]\n",
    "def rule_large(x):\n",
    "    if x == 0:\n",
    "        return 0.0\n",
    "    elif x == 1:\n",
    "        return 1/3\n",
    "    else:\n",
    "        return 2/3\n",
    "    \n",
    "vocab_list_base = [\n",
    "    'sound', 'prefix', 'adjective', 'verb', 'noun', 'word', 'Ngram', 'conversation', 'syllable'\n",
    "]\n",
    "def rule_base(x):\n",
    "    if x == 0:\n",
    "        return 0.0\n",
    "    elif x == 1:\n",
    "        return 1/64\n",
    "    else:\n",
    "        return 1/32\n",
    "    \n",
    "y_preds_question_type_spelling = (\n",
    "    test['question_title'].apply(\n",
    "        lambda x: sum([x.count(vocab) for vocab in vocab_list_large])\n",
    "    ) + test['question_body'].apply(\n",
    "        lambda x: sum([x.count(vocab) for vocab in vocab_list_large])\n",
    "    )).apply(rule_large) + (test['question_title'].apply(\n",
    "        lambda x: sum([x.count(vocab) for vocab in vocab_list_base])\n",
    "    ) + test['question_body'].apply(\n",
    "        lambda x: sum([x.count(vocab) for vocab in vocab_list_base])\n",
    "    )).apply(rule_base)\n",
    "\n",
    "stackexchange_particles = test['url'].apply(\n",
    "    lambda x:(('ell.stackexchange.com' in x) or ('english.stackexchange.com' in x))\n",
    ").tolist()\n",
    "spelling=[]\n",
    "for x in stackexchange_particles:\n",
    "    if x:\n",
    "        spelling.append(1/6)\n",
    "    else:\n",
    "        spelling.append(0.)\n",
    "        \n",
    "y_preds_question_type_spelling = y_preds_question_type_spelling + np.array(spelling)\n",
    "\n",
    "submission['question_type_spelling'] = y_preds_question_type_spelling\n",
    "submission.loc[test['category']!='CULTURE', 'question_type_spelling'] = 0.0\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "00eeda7766994d66b795e4e4dd67aca7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_06ddd7551b864d0d902bcbc43faa8ef8",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_dd0858b9befb470e84c8b647568746e2",
       "value": " 443/443 [23:52&lt;00:00,  3.23s/it, train_loss=0.56]"
      }
     },
     "06ddd7551b864d0d902bcbc43faa8ef8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "07d1569ccde5420d8d64b5ada9038233": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "08355cb655144ecab55bdbbac074a523": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_dfd215e4dadd4d5cad4a147d5af744f8",
        "IPY_MODEL_55e457580c5b4a709417a838f3ab6cdb"
       ],
       "layout": "IPY_MODEL_96413a118c254f88acf9f0ca8a2b91f0"
      }
     },
     "0c43874c192b423db0c505a400060dbf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_7e492a3862514d47be65824de9d2fac9",
        "IPY_MODEL_1e7b2a9dc3094df7913cb6c201b4fbad"
       ],
       "layout": "IPY_MODEL_ef69598239394da3a431513afe10b292"
      }
     },
     "14323c8bb9174f5197d9cc2470992711": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_293cc8f6531042d1b0d52a07846bbd81",
       "max": 443,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_7e89688fc7e04dabb64699230c5a8cf4",
       "value": 443
      }
     },
     "14e2f87a6acb4e69914ba37c8377a277": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1616ff7f291b48829838e769818e92b5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "16de4ed6cb4f4350a00529b9125d1e3c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1c0fc3e276ce4604901a10f0253450c3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1c42d47bdba14c6eaa2d80488ab5338e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_f17c22a7f7eb4a47917442fd1b3ae5c4",
       "max": 443,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_57ff83d7bd5c460ab5f2580d25bc291f",
       "value": 443
      }
     },
     "1d57a176b4aa4ce49cecdba19e3362b4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_dff5a3fcc98f4e1a980e8e461ca435b9",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_627690c977db4e9587c32ca34fb95bf7",
       "value": 1
      }
     },
     "1e7b2a9dc3094df7913cb6c201b4fbad": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_1c0fc3e276ce4604901a10f0253450c3",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_928783778dbf42cf8fae9bdddcecc71e",
       "value": " 443/443 [23:52&lt;00:00,  3.23s/it, train_loss=0.572]"
      }
     },
     "1ef6fd48f79c45f9a501e975ee544a64": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_8895b9b2f7734a518511876db7c7642b",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_867ec025b51d4ac79bd9ea904bdee8e0",
       "value": " 443/443 [18:12&lt;00:00,  2.47s/it, train_loss=0.75]"
      }
     },
     "2038afd2d932447cb0ea32e199b0741d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2326183582334fe3b849fbeb5b2b37fb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c387e66eb2814789a549aa8198e2ae72",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_7ff42d71b0494dee8582583fdc7f4c56",
       "value": " 443/443 [11:30&lt;00:00,  1.56s/it, train_loss=0.548]"
      }
     },
     "2648061ca3ba46c7b3788ec907621c2a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "271f9b929d534bdca8e0af6819450c43": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "293cc8f6531042d1b0d52a07846bbd81": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2de7e50b03f646148e56125819f6a957": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2e64ecd21dee42c1981f9c0d60e762b7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_14323c8bb9174f5197d9cc2470992711",
        "IPY_MODEL_8887b1fd36804dfa9e036e3d9d0d2511"
       ],
       "layout": "IPY_MODEL_3861c9ec79994a029c983f40c3abd284"
      }
     },
     "313c34f1271e4627ae6b4d361f7cf03e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c8e6dd8873374e1e9f03e65e79d326e6",
       "max": 443,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_cefa82bc00ba4391a8d05a6ef05648af",
       "value": 443
      }
     },
     "3861c9ec79994a029c983f40c3abd284": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "38b0840c231d4375be07ac594618542a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3cbf067d4d764549bc1283b49260fc1c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3dd65eac87ea4ed0af01853910d58722": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_9450f414437b40eeab2ae092d40839a5",
        "IPY_MODEL_f50e1e29481940c5959b1c0822329c1e"
       ],
       "layout": "IPY_MODEL_dc9c613091794bd6b5cb9f75039c6701"
      }
     },
     "449f0baa4f494c8283c09a92893a3723": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "4786fe8b91e6464191b5e9750e2ff98f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "488798b62afc4fa78cd3fd9be622bf67": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "4d29e65c4e874794a420f17d05ff40de": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_50d453a600754ffc80bb738ac53f53fa",
        "IPY_MODEL_9e195bb1586c405f9a80cb826b091030"
       ],
       "layout": "IPY_MODEL_c2bde7f1a1c64687ae613ab9853acb00"
      }
     },
     "4d9696f6ce714edfb7ac839744c96034": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4eed7710874145e88b9402bc4d3c235a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_4d9696f6ce714edfb7ac839744c96034",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_dfe27255bc2c433eadd8105f3d9d2372",
       "value": " 443/443 [23:49&lt;00:00,  3.23s/it, train_loss=0.728]"
      }
     },
     "50d453a600754ffc80bb738ac53f53fa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_38b0840c231d4375be07ac594618542a",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_5f62279d5a9a49ef85ec593fc62875c1",
       "value": 1
      }
     },
     "52b35fbf3a774b2f87a014eeaa4bc8cf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "535cc715f1af45719397298791d66bc7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "5434f01922234a1aa265ed7947f84c99": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_6dd0c1b6e60948daa0d8dfc108e62e9f",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_1616ff7f291b48829838e769818e92b5",
       "value": " 443/443 [23:50&lt;00:00,  3.23s/it, train_loss=0.707]"
      }
     },
     "55e457580c5b4a709417a838f3ab6cdb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_898535e349e34d9d955c762232ab53fc",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_f171f271958d42a4997672352654f13a",
       "value": " 443/443 [18:11&lt;00:00,  2.46s/it, train_loss=0.6]"
      }
     },
     "57ff83d7bd5c460ab5f2580d25bc291f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "5b2536c1106a4c8495ea11eb3a57ad91": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_6f0244beec444d3ba6df9e8226a82e29",
        "IPY_MODEL_4eed7710874145e88b9402bc4d3c235a"
       ],
       "layout": "IPY_MODEL_14e2f87a6acb4e69914ba37c8377a277"
      }
     },
     "5f62279d5a9a49ef85ec593fc62875c1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "627690c977db4e9587c32ca34fb95bf7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "65765507298e4e9793c6e9dc277ba232": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "65b7478a2b044419b980bac68dc0174f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_cdea62b2bace49dab353472030fcd9e6",
       "max": 443,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_81ae535e6a104d01aad300a98b0f122e",
       "value": 443
      }
     },
     "6c747f1c850d471fb3a43647bf8b10ed": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6c92c21ed44b43ab8998bd3d410d53cb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "6dd0c1b6e60948daa0d8dfc108e62e9f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6f0244beec444d3ba6df9e8226a82e29": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_16de4ed6cb4f4350a00529b9125d1e3c",
       "max": 443,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_6c92c21ed44b43ab8998bd3d410d53cb",
       "value": 443
      }
     },
     "72e9d1c7ba9d4a6aab856ce7b3d363ed": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7c47a1c948cf49b7813b12090a0004b2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_90a07e07fc9e4f65a924d4ad92bdf6ee",
        "IPY_MODEL_00eeda7766994d66b795e4e4dd67aca7"
       ],
       "layout": "IPY_MODEL_2de7e50b03f646148e56125819f6a957"
      }
     },
     "7e3c6d82533b445bb805c45994d6dd2f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7e492a3862514d47be65824de9d2fac9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_4786fe8b91e6464191b5e9750e2ff98f",
       "max": 443,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_90b39de233ef4b9da2fe71f67d5d6681",
       "value": 443
      }
     },
     "7e89688fc7e04dabb64699230c5a8cf4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "7ff42d71b0494dee8582583fdc7f4c56": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "81ae535e6a104d01aad300a98b0f122e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "820bd4dc7c5e4f3abfe86651586cc902": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_1d57a176b4aa4ce49cecdba19e3362b4",
        "IPY_MODEL_91776c49ebfd4bf4b5d53bafe7abb0f9"
       ],
       "layout": "IPY_MODEL_d8d4827b9373412a8574de9fd7c93583"
      }
     },
     "867ec025b51d4ac79bd9ea904bdee8e0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "8887b1fd36804dfa9e036e3d9d0d2511": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_f05ff02a7d1540da89e117587ad0b143",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_65765507298e4e9793c6e9dc277ba232",
       "value": " 443/443 [18:45&lt;00:00,  2.54s/it, train_loss=0.7]"
      }
     },
     "8895b9b2f7734a518511876db7c7642b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "898535e349e34d9d955c762232ab53fc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "90a07e07fc9e4f65a924d4ad92bdf6ee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_3cbf067d4d764549bc1283b49260fc1c",
       "max": 443,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_07d1569ccde5420d8d64b5ada9038233",
       "value": 443
      }
     },
     "90b39de233ef4b9da2fe71f67d5d6681": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "91776c49ebfd4bf4b5d53bafe7abb0f9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_feedcce35c1f4870a871ffef869560f3",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_52b35fbf3a774b2f87a014eeaa4bc8cf",
       "value": " 40/? [00:32&lt;00:00,  1.23it/s]"
      }
     },
     "928783778dbf42cf8fae9bdddcecc71e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "9450f414437b40eeab2ae092d40839a5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_6c747f1c850d471fb3a43647bf8b10ed",
       "max": 443,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_9b0af71c7cc54bbc93cfb1ba55c5b2b6",
       "value": 443
      }
     },
     "96413a118c254f88acf9f0ca8a2b91f0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "964c94a49bd0474890b35d45c5147bac": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9b0af71c7cc54bbc93cfb1ba55c5b2b6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "9da4cf18f6594f63b0bbdf803463b8b4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_964c94a49bd0474890b35d45c5147bac",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_535cc715f1af45719397298791d66bc7",
       "value": " 443/443 [23:49&lt;00:00,  3.23s/it, train_loss=0.704]"
      }
     },
     "9e195bb1586c405f9a80cb826b091030": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2038afd2d932447cb0ea32e199b0741d",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_2648061ca3ba46c7b3788ec907621c2a",
       "value": " 40/? [00:31&lt;00:00,  1.27it/s]"
      }
     },
     "9ffe7a1b7b064c53a1b8bb38537c69a2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a1aea9ac2c9a4af2828550a8c4f50798": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a6712f8c70c54893a8543be0baf179f8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_bef478868ad644d586fb6a226cb54729",
        "IPY_MODEL_9da4cf18f6594f63b0bbdf803463b8b4"
       ],
       "layout": "IPY_MODEL_9ffe7a1b7b064c53a1b8bb38537c69a2"
      }
     },
     "be27018963564854871208aca226355f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bef478868ad644d586fb6a226cb54729": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d45b7d0ac36a4de18f837bd23caa669e",
       "max": 443,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_488798b62afc4fa78cd3fd9be622bf67",
       "value": 443
      }
     },
     "c2bde7f1a1c64687ae613ab9853acb00": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c387e66eb2814789a549aa8198e2ae72": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c8e6dd8873374e1e9f03e65e79d326e6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cdea62b2bace49dab353472030fcd9e6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cefa82bc00ba4391a8d05a6ef05648af": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "d25dca6766d44f6d9855ef87c931cc38": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "d45b7d0ac36a4de18f837bd23caa669e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d8d4827b9373412a8574de9fd7c93583": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "dc9c613091794bd6b5cb9f75039c6701": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "dd0858b9befb470e84c8b647568746e2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "dfd215e4dadd4d5cad4a147d5af744f8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_271f9b929d534bdca8e0af6819450c43",
       "max": 443,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_d25dca6766d44f6d9855ef87c931cc38",
       "value": 443
      }
     },
     "dfe27255bc2c433eadd8105f3d9d2372": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "dff5a3fcc98f4e1a980e8e461ca435b9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e41b38992adf47ed9e2d4dffc8f9e1e3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_1c42d47bdba14c6eaa2d80488ab5338e",
        "IPY_MODEL_1ef6fd48f79c45f9a501e975ee544a64"
       ],
       "layout": "IPY_MODEL_a1aea9ac2c9a4af2828550a8c4f50798"
      }
     },
     "ec47d3ecf47d424b98c2c00b83089e21": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_313c34f1271e4627ae6b4d361f7cf03e",
        "IPY_MODEL_5434f01922234a1aa265ed7947f84c99"
       ],
       "layout": "IPY_MODEL_7e3c6d82533b445bb805c45994d6dd2f"
      }
     },
     "ef69598239394da3a431513afe10b292": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f05ff02a7d1540da89e117587ad0b143": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f171f271958d42a4997672352654f13a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "f17c22a7f7eb4a47917442fd1b3ae5c4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f50e1e29481940c5959b1c0822329c1e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_72e9d1c7ba9d4a6aab856ce7b3d363ed",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_449f0baa4f494c8283c09a92893a3723",
       "value": " 443/443 [23:48&lt;00:00,  3.22s/it, train_loss=0.702]"
      }
     },
     "fd2d917bf67e4df68f02c13a014e7454": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_65b7478a2b044419b980bac68dc0174f",
        "IPY_MODEL_2326183582334fe3b849fbeb5b2b37fb"
       ],
       "layout": "IPY_MODEL_be27018963564854871208aca226355f"
      }
     },
     "feedcce35c1f4870a871ffef869560f3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
